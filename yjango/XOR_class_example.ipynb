{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.deprecated as deprecated\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FNN(object):\n",
    "    \n",
    "    \"\"\"Build a general FeedForward neural network\n",
    "    Parameters\n",
    "    ----------\n",
    "    learning_rate : float\n",
    "    drop_out : float\n",
    "    Layers : list\n",
    "      The number of layers\n",
    "    N_hidden : list\n",
    "      The numbers of nodes in layers\n",
    "    D_input : int\n",
    "      Input dimension\n",
    "    D_label : int\n",
    "      Label dimension\n",
    "    Task_type : string\n",
    "      'regression' or 'classification'\n",
    "    L2_lambda : float\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate, drop_keep, Layers, N_hidden,\n",
    "                 D_input, D_label, Task_type='regression', L2_lambda=0.0):\n",
    "        # 全部共有属性\n",
    "        self.learning_rate = learning_rate\n",
    "        self.drop_keep = drop_keep\n",
    "        self.Layers = Layers\n",
    "        self.N_hidden = N_hidden\n",
    "        self.D_input = D_input\n",
    "        self.D_label = D_label\n",
    "        # 类型控制 loss 函数的选择\n",
    "        self.Task_type = Task_type\n",
    "        # L2 regularization 的惩罚强弱, 过高会使输出都拉向 0\n",
    "        self.L2_lambda = L2_lambda\n",
    "        # 用于存放所累计的每层 L2 regularization\n",
    "        self.l2_penalty = tf.constant(0.0)\n",
    "        \n",
    "        # 用于生成 tensorflow 缩放图, 括号里起名字\n",
    "        with tf.name_scope('Input'):\n",
    "            self.inputs = tf.placeholder(tf.float32, [None, D_input], name='inputs')\n",
    "        with tf.name_scope('Label'):\n",
    "            self.labels = tf.placeholder(tf.float32, [None, D_label], name='labels')\n",
    "        with tf.name_scope('keep_rate'):\n",
    "            self.drop_keep_rate = tf.placeholder(tf.float32, name='dropout_keep')\n",
    "        #初始化的时候直接生成\n",
    "        self.build('F')\n",
    "    \n",
    "    def weight_init(self, shape):\n",
    "        # shape: list [in_dim, out_dim]\n",
    "        # can change initialization here\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def bias_init(self, shape):\n",
    "        # can chage initialization here\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def variable_summaries(self, var, name):\n",
    "        with tf.name_scope(name + '_summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            deprecated.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope(name + '_stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        # 记录每次训练后变量的数值变化\n",
    "        deprecated.scalar_summary('_stddev/' + name, stddev)\n",
    "        deprecated.scalar_summary('_max/' + name, tf.reduce_max(var))\n",
    "        deprecated.scalar_summary('_min/' + name, tf.reduce_min(var))\n",
    "        deprecated.histogram_summary(name, var)\n",
    "    \n",
    "    # 用于建立每层的函数\n",
    "    def layer(self, in_tensor, in_dim, out_dim, layer_name, act=tf.nn.relu):\n",
    "        with tf.name_scope(layer_name):\n",
    "            with tf.name_scope(layer_name + '_weights'):\n",
    "                # 用所建立的 weight_init 函数进行初始化\n",
    "                weights = self.weight_init([in_dim, out_dim])\n",
    "                # 存放每一个权重 W\n",
    "                self.W.append(weights)\n",
    "                # 对权重进行统计\n",
    "                self.variable_summaries(weights, layer_name + '/weights')\n",
    "            with tf.name_scope(layer_name + '_biases'):\n",
    "                biases = self.bias_init([out_dim])\n",
    "                self.b.append(biases)\n",
    "                self.variable_summaries(biases, layer_name + '/biases')\n",
    "            with tf.name_scope(layer_name + '_Wx_plus_b'):\n",
    "                # 计算 Wx+b\n",
    "                pre_activate = tf.matmul(in_tensor, weights) + biases\n",
    "                # 记录直方图\n",
    "                deprecated.histogram_summary(layer_name + '/pre_activations', pre_activate)\n",
    "            activations = act(pre_activate, name='activation')\n",
    "            deprecated.histogram_summary(layer_name + '/activations', activations)\n",
    "        # 最终返回该层的输出, 以及权重 W 的 L2\n",
    "        return activations, tf.nn.l2_loss(weights)\n",
    "    \n",
    "    # 创建 dropout 层的函数\n",
    "    def drop_layer(self, in_tensor):\n",
    "        dropped = tf.nn.dropout(in_tensor, self.drop_keep_rate)\n",
    "        return dropped\n",
    "    \n",
    "    # 构建网络\n",
    "    def build(self, prefix):\n",
    "        # incoming 也代表当前 tensor 的流动位置\n",
    "        incoming = self.inputs\n",
    "        # 如果没有隐藏层\n",
    "        if self.Layers != 0:\n",
    "            layer_nodes = [self.D_input] + self.N_hidden\n",
    "        else:\n",
    "            layer_nodes = [self.D_input]\n",
    "        \n",
    "        # hid_layers 用于存储所有隐藏层的输出\n",
    "        self.hid_layers = []\n",
    "        # W 用于存储所有层的权重\n",
    "        self.W = []\n",
    "        # b 用于存储所有层的偏移\n",
    "        self.b = []\n",
    "        # total_l2 用于存储所有层的 L2\n",
    "        self.total_l2 = []\n",
    "        # drop 存储 dropout 后的输出\n",
    "        self.drop = []\n",
    "        \n",
    "        # 开始叠加隐藏层. 这跟千层饼没什么区别\n",
    "        for l in range(self.Layers):\n",
    "            # 使用刚才编写的函数来建立层, 并更新 incoming 的位置\n",
    "            incoming, l2_loss = self.layer(incoming, layer_nodes[l], layer_nodes[l+1], prefix + '_hid_' + str(l+1), act=tf.nn.relu)\n",
    "            # 累计 l2\n",
    "            self.total_l2.append(l2_loss)\n",
    "            # 输出一些信息, 让我们知道网络在建造中做了什么\n",
    "            print('Add dense layer: relu with drop_keep: %s' % self.drop_keep)\n",
    "            print('    %sD --> %sD' % (layer_nodes[l], layer_nodes[l+1]))\n",
    "            # 存储所有隐藏层的输出\n",
    "            self.hid_layers.append(incoming)\n",
    "            # 加入 dropout 层\n",
    "            incoming = self.drop_layer(incoming)\n",
    "            # 存储所有 dropout 后的输出\n",
    "            self.drop.append(incoming)\n",
    "        \n",
    "        # 输出层的建立. 输出层需要特别对待的原因是输出层的 activation function 要根据任务来变.\n",
    "        # 回归任务的话, 下面用的是 tf.identity, 也就是没有 activation function\n",
    "        if self.Task_type == 'regression':\n",
    "            out_act = tf.identity\n",
    "        else:\n",
    "            # 分类任务使用 softmax 来拟合概率\n",
    "            out_act = tf.nn.softmax\n",
    "        self.output, l2_loss = self.layer(incoming, layer_nodes[-1], self.D_label, layer_name='output', act=out_act)\n",
    "        self.total_l2.append(l2_loss)\n",
    "        print('Add output layer: linear')\n",
    "        print('    %sD --> %sD' % (layer_nodes[-1], self.D_label))\n",
    "        \n",
    "        # l2 loss 的缩放图\n",
    "        with tf.name_scope('total_l2'):\n",
    "            for l2 in self.total_l2:\n",
    "                self.l2_penalty += l2\n",
    "            deprecated.scalar_summary('l2_penalty', self.l2_penalty)\n",
    "            \n",
    "        # 不同任务的 loss\n",
    "        # 若为回归, 则 loss 是用于判断所有预测值和实际值差别的函数\n",
    "        if self.Task_type == 'regression':\n",
    "            with tf.name_scope('SSE'):\n",
    "                self.loss = tf.reduce_mean((self.output - self.labels)**2)\n",
    "                deprecated.scalar_summary('loss', self.loss)\n",
    "        else:\n",
    "            # 若为分类, cross entropy 的 loss function\n",
    "            entropy = tf.nn.softmax_cross_entropy_with_logits(self.output, self.labels)\n",
    "            with tf.name_scope('cross entropy'):\n",
    "                self.loss = tf.reduce_mean(entropy)\n",
    "                deprecated.scalar_summary('loss', self.loss)\n",
    "            with tf.name_scope('accuracy'):\n",
    "                correct_prediction = tf.equal(tf.argmax(self.output, 1), tf.argmax(self.labels, 1))\n",
    "                self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "                deprecated.scalar_summary('accuracy', self.accuracy)\n",
    "        \n",
    "        # 整合所有 loss, 形成最终 loss\n",
    "        with tf.name_scope('total_loss'):\n",
    "            self.total_loss = self.loss + self.l2_penalty * self.L2_lambda\n",
    "            deprecated.scalar_summary('total_loss', self.total_loss)\n",
    "        # 训练操作\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_step = tf.train.AdamOptimizer(self.learning_rate).minimize(self.total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 数据准备\n",
    "inputs = [ [0, 0], [0, 1], [1, 0], [1, 1] ]\n",
    "outputs = [0, 1, 1, 0]\n",
    "X = np.array(inputs).reshape((4, 1, 2)).astype('int16')\n",
    "Y = np.array(outputs).reshape((4, 1, 1)).astype('int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成网络实例\n",
    "ff = FNN(learning_rate=1e-3, drop_keep=1.0, Layers=1, N_hidden=[2], D_input=2, D_label=1, Task_type='regression', L2_lambda=1e-2)\n",
    "\n",
    "# 加载会话 session\n",
    "sess = tf.InteractiveSession()\n",
    "tf.initialize_all_variables().run()\n",
    "# 用于将所有 summary 合成一个 ops\n",
    "merged = deprecated.merge_all_summaries()\n",
    "# 用于记录训练中内容. 前一个参数是记录地址, 后一个参数是记录的 graph\n",
    "train_writer = tf.summary.FileWriter('log' + '/train', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练前的权重\n",
    "# 获得输出\n",
    "W0 = sess.run(ff.W[0])\n",
    "W1 = sess.run(ff.W[1])\n",
    "# 显示\n",
    "print('W_0:\\n%s' % sess.run(ff.W[0]))\n",
    "print('W_1:\\n%s' % sess.run(ff.W[1]))\n",
    "\n",
    "plt.scatter([1, 1, 5], [1, 3, 2], color=['red', 'red', 'blue'], s=200, alpha=0.4, marker='o')\n",
    "plt.scatter([3, 3], [1, 3], color=['green', 'green'], s=200, alpha=0.4, marker='o')\n",
    "\n",
    "plt.plot([1,3],[1,1],color='orange',linewidth=abs(W0[0,0])*10)\n",
    "plt.annotate('%0.2f' %W0[0,0],xy=(2, 1.0))\n",
    "\n",
    "plt.plot([1,3],[3,1],color='blue',linewidth=abs(W0[1,0])*10)\n",
    "plt.annotate('%0.2f' %W0[1,0],xy=(1.5, 1.5))\n",
    "\n",
    "plt.plot([1,3],[1,3],color='blue',linewidth=abs(W0[0,1])*10)\n",
    "plt.annotate('%0.2f' %W0[0,1],xy=(1.5, 2.5))\n",
    "\n",
    "plt.plot([1,3],[3,3],color='orange',linewidth=abs(W0[1,1])*10)\n",
    "plt.annotate('%0.2f' %W0[1,1],xy=(2, 3))\n",
    "\n",
    "plt.plot([3,5],[1,2],color='blue',linewidth=abs(W1[0])*10)\n",
    "plt.annotate('%0.2f' %W1[0],xy=(4, 1.5))\n",
    "\n",
    "plt.plot([3,5],[3,2],color='blue',linewidth=abs(W1[1])*10)\n",
    "plt.annotate('%0.2f' %W1[1],xy=(4, 2.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练前的输出\n",
    "feed_dict = { ff.inputs: X.reshape((4, 2)), ff.drop_keep_rate: 1.0 }\n",
    "print('feed_dict: %s' % feed_dict )\n",
    "pY = sess.run(ff.output, feed_dict=feed_dict)\n",
    "print(pY)\n",
    "plt.scatter([0, 1, 2, 3], pY, color=['red', 'green', 'blue', 'black'], s=25, alpha=0.4, marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练前隐藏层的输出\n",
    "pY = sess.run(ff.hid_layers[0], feed_dict={ ff.inputs: X.reshape((4, 2)), ff.drop_keep_rate: 1.0})\n",
    "print(pY)\n",
    "plt.scatter(pY[:, 0], pY[:, 1], color=['red', 'green', 'blue', 'black'], s=25, alpha=0.4, marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练并记录\n",
    "k = 0.0\n",
    "# print('%s' % [merged, ff.train_step])\n",
    "# print(merged)\n",
    "# print(ff.train_step)\n",
    "for i in range(100000):\n",
    "    k += 1\n",
    "    if k % 5000 == 0:\n",
    "        print(k)\n",
    "#     print('%s' % [merged, ff.train_step])\n",
    "    summary, _ = sess.run([merged, ff.train_step], feed_dict={ ff.inputs: X.reshape((4, 2)), ff.labels: Y.reshape((4, 1)), ff.drop_keep_rate: 1.0 })\n",
    "    train_writer.add_summary(summary, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 训练后的权重\n",
    "\n",
    "W0=sess.run(ff.W[0])\n",
    "W1=sess.run(ff.W[1])\n",
    "print('W_0:\\n%s' %sess.run(ff.W[0]))\n",
    "print('W_1:\\n%s' %sess.run(ff.W[1]))\n",
    "\n",
    "plt.scatter([1,1,5],[1,3,2],color=['red','red','blue'],s=200,alpha=0.4,marker='o')\n",
    "plt.scatter([3,3],[1,3],color=['green','green'],s=200,alpha=0.4,marker='o')\n",
    "\n",
    "plt.plot([1,3],[1,1],color='orange',linewidth=abs(W0[0,0])*10)\n",
    "plt.annotate('%0.2f' %W0[0,0],xy=(2, 1.0))\n",
    "\n",
    "plt.plot([1,3],[3,1],color='blue',linewidth=abs(W0[1,0])*10)\n",
    "plt.annotate('%0.2f' %W0[1,0],xy=(1.5, 1.5))\n",
    "\n",
    "plt.plot([1,3],[1,3],color='blue',linewidth=abs(W0[0,1])*10)\n",
    "plt.annotate('%0.2f' %W0[0,1],xy=(1.5, 2.5))\n",
    "\n",
    "plt.plot([1,3],[3,3],color='orange',linewidth=abs(W0[1,1])*10)\n",
    "plt.annotate('%0.2f' %W0[1,1],xy=(2, 3))\n",
    "\n",
    "plt.plot([3,5],[1,2],color='blue',linewidth=abs(W1[0])*10)\n",
    "plt.annotate('%0.2f' %W1[0],xy=(4, 1.5))\n",
    "\n",
    "plt.plot([3,5],[3,2],color='blue',linewidth=abs(W1[1])*10)\n",
    "plt.annotate('%0.2f' %W1[1],xy=(4, 2.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练后的输出\n",
    "pY=sess.run(ff.output,feed_dict={ff.inputs:X.reshape((4,2)),ff.drop_keep_rate:1.0})\n",
    "print(pY)\n",
    "plt.scatter([0,1,2,3],pY,color=['red','green','blue','black'],s=25,alpha=0.4,marker='o')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
