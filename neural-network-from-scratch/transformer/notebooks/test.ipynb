{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4b50b06-3d4a-4059-b746-f9ae962a88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "462d02f4-52dc-4166-9c57-b0a282e5013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "import sys  \n",
    "\n",
    "# Get my_package directory path from Notebook\n",
    "parent_dir = str(Path().resolve().parents[0])\n",
    "\n",
    "# Add to sys.path\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f778f7aa-f3e3-4cc9-b9a4-e80dacc8b5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/taot/data/ml_data/auto_download/huggingface\n"
     ]
    }
   ],
   "source": [
    "hf_home = os.environ[\"HF_HOME\"]\n",
    "print(hf_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1c4957-9c0b-4eed-81bc-4c61fe1c96d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 25984574\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3981\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds = datasets.load_dataset(\"wmt/wmt19\", \"zh-en\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb000623-17cd-48a4-b2ef-7b0a5e600549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f947eda-8ca5-47a8-a90a-693f1f0e0e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"train\" in ds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8088ea1-992b-4b74-9f21-7a774160ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c20aff1-73d9-4439-96a7-e23067435443",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d84e59b-196c-4bb4-b3a9-afb9bfd22b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2de83112-b985-43c6-b814-b2b0c831aa17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][1][\"translation\"][\"zh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa88ba9-aa7d-4d92-ac22-5730d1037965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][1][\"translation\"][\"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9916a3b5-5d51-4e62-a86c-a15c8cdffb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b8c20-1b8e-4cac-a198-ece9a5338f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ds[\"train\"]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce53cd-6c85-4cb6-951b-f03ac784c4ab",
   "metadata": {},
   "source": [
    "# HuggingFace Tokenizer Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5da9057b-f74a-4191-a335-57f65a2e2598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23a50a1b-751c-4be6-aa11-aabd07ab0226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e55baa52f34084a6c51cc4dfc26854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7a4030c19549a1987e25681f74956e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772c96ffb5da46e183a4c58ad8869e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2b07197-8f66-481c-8100-d2313c7fab50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Using a Transformer network is simple\"\n",
    "# sequence = \"PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb6231da-22f8-4d39-b69d-7acf5fb0f6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e4e22bc-1d1b-4c65-8371-4c866aeb3407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokens_for_model = tokenizer.prepare_for_model(ids)\n",
    "print(tokens_for_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd845f92-14ed-46a3-b138-28f971ba64ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Using a Transformer network is simple [SEP]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens_for_model[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03df8aac-cd5a-46ff-89d3-e582df5a37de",
   "metadata": {},
   "source": [
    "# Experiment Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b2f9f6b5-cae4-4475-b1b4-90eeb1e651e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67299043-5ec2-4d75-b276-a9e23ee7b06f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lang'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtrain\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mget_or_build_tokenizer(config, ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/github/playground/neural-network-from-scratch/transformer/train.py:30\u001b[0m, in \u001b[0;36mget_or_build_tokenizer\u001b[0;34m(config, ds, lang)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_or_build_tokenizer\u001b[39m(config: Dict[\u001b[38;5;28mstr\u001b[39m, Any], ds: datasets\u001b[38;5;241m.\u001b[39mDataset, lang: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tokenizer:\n\u001b[0;32m---> 30\u001b[0m     tokenizer_path \u001b[38;5;241m=\u001b[39m Path(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mformat(lang))\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_or_build_tokenizer: tokenizer_path = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tokenizer_path\u001b[38;5;241m.\u001b[39mexists():\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lang'"
     ]
    }
   ],
   "source": [
    "import train\n",
    "from config import config\n",
    "\n",
    "tokenizer = train.get_or_build_tokenizer(config, ds, \"en\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
